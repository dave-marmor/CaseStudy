{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "SalesHeader = pd.read_csv('SalesHeader.csv')\n",
    "SalesItem = pd.read_csv('SalesItem.csv')\n",
    "Location = pd.read_csv('Location.csv')\n",
    "Customer = pd.read_csv('Customer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM NUMBER 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL QUERY, where:\n",
    "si: SalesItem\n",
    "cs: Customer\n",
    "sh: SalesHeader\n",
    "lc: Location\n",
    "\n",
    "#1) create our target variable \"TotalAmtPaid\" and select to display both customer name and total amount paid as those\n",
    "# are the final two columns we are interested in:\n",
    "select cs.\"CustomerName\", sum((si.\"AmtPaidPerCase\"*si.\"Quantity\")) as TotalAmtPaid  from customers cs\n",
    "\n",
    "#2) proceed to join all data frames in consecutive order\n",
    "   left join salesheader sh on cs.\"CustomerID\" = sh.\"CustomerID\"\n",
    "\n",
    "left join items si on si.\"OrderID\"=sh.\"OrderID\"\n",
    "\n",
    "left join location lc on lc.\"LocationID\" = sh.\"ShipToID\"\n",
    "\n",
    "#3) introduce the limitations on the type of data we are interested in:\n",
    "where lc.\"State\"='TX' and lc.\"LocationType\" = 'Store'\n",
    "  \n",
    "    and sh.\"OrderDate\" >= '2018-09-01' AND sh.\"OrderDate\" < '2018-10-01'\n",
    "#4) finally groupby customer name to aggregate all individual amounts paid per customer and limit the resulting dataframe\n",
    "#to a length of 10 rows.\n",
    "group by cs.\"CustomerName\"\n",
    "limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM NUMBER 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalAmtPaid</th>\n",
       "      <th>CustomerName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.92</td>\n",
       "      <td>Fun Superstore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TotalAmtPaid    CustomerName\n",
       "0         68.92  Fun Superstore"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1Find out how much an order cost in total\n",
    "SalesItem['TotalAmtPaid'] = SalesItem.Quantity * SalesItem.AmtPaidPerCase\n",
    "\n",
    "#2 Merge data frame with date ordered, total cost and customer\n",
    "ItemHead = pd.merge(SalesItem,SalesHeader)\n",
    "ItemHeadCust = pd.merge(ItemHead,Customer)\n",
    "\n",
    "#3 Subset main dataframe for Sept 2018 using regex:\n",
    "IteamHeadCust = ItemHeadCust[ItemHeadCust.OrderDate.str.contains('09/[0-9]+/2018')]\n",
    "\n",
    "#4 Merge location data frame with other 3 merged dataframes on ShipToID as we only want stores:\n",
    "Alldata = pd.merge(ItemHeadCust, Location, left_on=  ['ShipToID'],\n",
    "                   right_on= ['LocationID'], \n",
    "                   how = 'left')\n",
    "\n",
    "#5 Subset location data for stores and TX only:\n",
    "Alldata = Alldata[(Alldata.LocationType == 'Store') & (Alldata.State == 'TX') ]\n",
    "\n",
    "#6 groupby customer name and sum total amount paid:\n",
    "Final = Alldata.groupby('CustomerName').agg(sum).reset_index()\n",
    "\n",
    "#6 Output top 10 customers with the highest total amount paid:\n",
    "Final[['TotalAmtPaid','CustomerName']].sort_values(by = 'TotalAmtPaid',ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM NUMBER 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>ShipFromID</th>\n",
       "      <th>ShipToID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00152369</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>159</td>\n",
       "      <td>174747</td>\n",
       "      <td>4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86592586</td>\n",
       "      <td>09/04/2018</td>\n",
       "      <td>74</td>\n",
       "      <td>45944</td>\n",
       "      <td>775553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00075396</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>2583</td>\n",
       "      <td>45944</td>\n",
       "      <td>253447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65324852</td>\n",
       "      <td>07/28/2018</td>\n",
       "      <td>9</td>\n",
       "      <td>145823</td>\n",
       "      <td>857456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00074474</td>\n",
       "      <td>08/14/2018</td>\n",
       "      <td>4564</td>\n",
       "      <td>685144</td>\n",
       "      <td>857456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00159447</td>\n",
       "      <td>01/03/2018</td>\n",
       "      <td>853</td>\n",
       "      <td>915354</td>\n",
       "      <td>954835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00654151</td>\n",
       "      <td>11/15/2018</td>\n",
       "      <td>2583</td>\n",
       "      <td>760613</td>\n",
       "      <td>15501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>02315454</td>\n",
       "      <td>09/04/2018</td>\n",
       "      <td>2583</td>\n",
       "      <td>233054</td>\n",
       "      <td>86592586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14881448</td>\n",
       "      <td>05/17/2018</td>\n",
       "      <td>6895</td>\n",
       "      <td>468871</td>\n",
       "      <td>929366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    OrderID   OrderDate  CustomerID  ShipFromID  ShipToID\n",
       "0  00152369  01/01/2018         159      174747      4236\n",
       "1  86592586  09/04/2018          74       45944    775553\n",
       "2  00075396  01/01/2018        2583       45944    253447\n",
       "3  65324852  07/28/2018           9      145823    857456\n",
       "4  00074474  08/14/2018        4564      685144    857456\n",
       "5  00159447  01/03/2018         853      915354    954835\n",
       "6  00654151  11/15/2018        2583      760613     15501\n",
       "7  02315454  09/04/2018        2583      233054  86592586\n",
       "9  14881448  05/17/2018        6895      468871    929366"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add0_remove_sub5(df):\n",
    "    #1Copy dataframe\n",
    "    df1 = df.copy()\n",
    "\n",
    "    #Convert all code IDs to strings:\n",
    "    df1['OrderID'] = [str(x) for x in df1['OrderID']]\n",
    "    \n",
    "    #Eliminate all below 5:\n",
    "    df1 = df1[df1['OrderID'].apply(lambda x: len(x)>4)]\n",
    "    \n",
    "    #Add 0s:\n",
    "    df1['OrderID'] = [(8-len(x))*'0'+x for x in df1['OrderID']]\n",
    "    return df1\n",
    "\n",
    "add0_remove_sub5(SalesHeader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM NUMBER 4:\n",
    "I would create a multiple linear regression model to predict the AmtPaidPerCase under the assumption that there is a relationship between the item ID, quantity ordered, distance between to and from locations and the type of customer. If R^2>0.7, I would use this model to predict the amount paid per case. The reason why I would use this method instead of random or mean imputation is because it seems to me that the amount paid per case should logically vary based on other features in the dataset. <br>\n",
    "Hot deck imputation could also be compared with regression based imputaion since it randomly choes a values from a sample that has similar values for other features. This is essentially a clustering method, which might actually yield better results depending on the relationships between the features and the amount paid per case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM NUMBER 5:\n",
    "#### 1) data/feature engineering: (python: pandas, reg.ex, numpy and seaborn)\n",
    "- Change the date into three different column features: day, month and year so as to accurately capture the relationship with time.<br>\n",
    "- Merge all dataframes so that we have 1 dataframe with all the information. Make sure to merge the location dataframe onto both shipIDfrom and shipIDto.<br>\n",
    "- Delete redundant information such as OrderID, customerID, locationID and ship to/from IDs since now our dataframes are merged.<br>\n",
    "- Create a couple of plots to better understand the data: distribution of total amount of items sold per day, matrix of lin. reg. plots between price and each feature and matrix of lin. reg. plots between features to identify auto-correlation<br>\n",
    "- Look at summary statistics in groupby dataframes, histograms and boxplots to identify outliers.<br>\n",
    "- If there is high auto-correlation, use VIF = 4 as a threshold to eliminate features.<br>\n",
    "- No dummifying needs to be done for ordinal or categorical variables.<br>\n",
    "\n",
    "#### 2) modeling: (python: sklearn (ensemble & model_selection))\n",
    "- Type of problem: supervised learning regression problem\n",
    "- Remarks about model types:<br>\n",
    "    - We don't have many features, so no issues with curse of dimensionality.<br>\n",
    "    - Deep learning models don't seem too useful here as the data-set is simplistic and predicting demand is extremely hard to pinpoint in retail even with ML/AI. <br>\n",
    "    - I don't think we necessarily have a linear relationship between features and predicted paid amount, so a model that can accomodate non-linear relationships might be good.<br>\n",
    "    - Might want a model that allows for feature interaction to better capture the relationships which might not necessarily be monotone.<br>\n",
    "    - Considering the type of data and the end goal of having the model be utilized by the business, it might be nice to have a certain amount of interpretability.<br>\n",
    "- I think a random forest model would fulfill the above confiditions.<br>\n",
    "- Would use the random forest regresion from sklearn with the randomized cross-validation search to accurately score the parameters chosen for that model. Depending on how many data points we have, we could have anywhere from 2 folds, to 100+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM NUMBER 6:\n",
    "\n",
    "Similar to the last part of my answer to problem 6, to best evaluate the model, I would use cross-validation (train test splt method) to ensure my model scoring is accurate.\n",
    "The number of folds would be determined by the amount of data avaialble where the more data points I have, the more folds I can afford to make and not run the risk of overfitting. I would use both an adjusted R^2 value as well as the RMSE to score my models and fine tune my tree's hyperparameters. R^2 measures the amount of variance in the target variable that is  explained by the model, whereas RMSE is the amount of error in the units of the target variabele. RMSE more simply put is the standard deviation of the unexplained variance. The combination of these provides a good overall scoring methodology that would be outputted during the grid CV hyperperameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBLEM NUMBER 7:\n",
    "Automating our dashboard comes down to streamlining the ETL process a.k.a. extracting, transforming and loading our data.<br>\n",
    "1) Extract the data, whether that be from a data lake, SQL (a relational database) or a data warehouse, I would use Spark to first access that data, extract it and partition if for processing.<br>\n",
    "2) Once the data has been extracted from it's main warehousing source I would have a series of dataframe transforming functions that would tackle the pulled data. Spark allows this to be done very quickly as it is a distributed engine (runs computations in parallel) and allows for the chaining of transforming functions.<br>\n",
    "3) As Spark is processing the data, I would write it into S3 (Amazon's Simple Storage Service would have be be leased) without having to setup a server. Databrick's platform as well as its API are fantastic for building out this Spark pipeline, which would also involve a financial investment.<br>\n",
    "4) At this point our data is ready to be fed into a BI tool, whether that be a model or a vizualization. A data inflow would trigger the events of steps 1 through 4 all over again. I would setup this trigger by having a set of conditions that need to be fulfilled before commencing the process.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
